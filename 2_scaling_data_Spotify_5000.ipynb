{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c7f733",
   "metadata": {},
   "source": [
    "# 2 â€” Scaling the Spotify Audio Features (Beginner-Friendly)\n",
    "\n",
    "**Why this notebook?**\n",
    "\n",
    "Clustering uses distances between songs. If some features have very large ranges or are strongly skewed, they can dominate the distance measurement.\n",
    "\n",
    "Here we compare common scalers and **see how scaling affects K-Means quality** on the Moosic dataset.\n",
    "\n",
    "\n",
    "**Youâ€™ll learn:**\n",
    "\n",
    "- When and why to scale features\n",
    "\n",
    "- What each scaler does (Standard/MinMax/Robust/Quantile)\n",
    "\n",
    "- How scaling changes clustering metrics for K-Means at a fixed k (we'll use k=20)\n",
    "\n",
    "\n",
    "> Tip: We keep `k=20` here so we isolate the effect of **scaling** only. In the next notebook, weâ€™ll tune `k`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344b599",
   "metadata": {},
   "source": [
    "## 0. Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Scalers & model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7,5)\n",
    "RNG = np.random.RandomState(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df03cc",
   "metadata": {},
   "source": [
    "## 1. Load the data and choose features\n",
    "Place your CSV at `../data/spotify_5000_songs.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33afb723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA = Path(\"../data/spotify_5000_songs.csv\")\n",
    "assert DATA.exists(), f\"Missing data at {DATA}. Place your CSV there.\"\n",
    "\n",
    "# Clean column names: collapse spaces and keep first token (handles 'name     ...' exports)\n",
    "def clean_col(c):\n",
    "    s = re.sub(r\"\\s+\", \" \", str(c)).strip()\n",
    "    return s.split(\" \")[0]\n",
    "\n",
    "df_raw = pd.read_csv(DATA)\n",
    "df = df_raw.copy()\n",
    "df.columns = [clean_col(c) for c in df.columns]\n",
    "\n",
    "FEATURES = ['danceability','energy','acousticness','instrumentalness','liveness','valence',\n",
    "            'tempo','speechiness','loudness','duration_ms','key','mode','time_signature']\n",
    "available = [c for c in FEATURES if c in df.columns]\n",
    "X = df[available].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "print(\"Using features:\", available)\n",
    "X.describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc44ed",
   "metadata": {},
   "source": [
    "## 2. Check distribution shapes (skew)\n",
    "Skewed features benefit from transformations that make them more symmetric (e.g., Quantile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab838e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skew = X.skew(numeric_only=True).sort_values(ascending=False)\n",
    "skew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d6878",
   "metadata": {},
   "source": [
    "*(Optional quick look)* Histograms for a couple of features before scaling.\n",
    "Use this to spot very skewed features like `acousticness` or `instrumentalness`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_to_plot = [c for c in ['energy','valence','tempo','duration_ms','acousticness','instrumentalness'] if c in X.columns][:3]\n",
    "for c in cols_to_plot:\n",
    "    plt.figure()\n",
    "    X[c].hist(bins=40)\n",
    "    plt.title(f'Histogram (raw): {c}')\n",
    "    plt.xlabel(c); plt.ylabel('count')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4143f8",
   "metadata": {},
   "source": [
    "## 3. Define scalers and a helper to score clusters\n",
    "Weâ€™ll fit **K-Means (k=20)** on each scaled matrix and compute three common metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SCALERS = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'QuantileTransformer': QuantileTransformer(output_distribution='normal', n_quantiles=min(1000, len(X)), random_state=42)\n",
    "}\n",
    "\n",
    "def kmeans_metrics(Xt, k=20, random_state=42):\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=random_state).fit(Xt)\n",
    "    labels = km.labels_\n",
    "    # If there's only one cluster (rare here), metrics are undefined\n",
    "    uniq = set(labels)\n",
    "    if len(uniq) < 2:\n",
    "        return {'silhouette': None, 'davies_bouldin': None, 'calinski_harabasz': None, 'inertia': float(km.inertia_)}\n",
    "    return {\n",
    "        'silhouette': float(silhouette_score(Xt, labels)),\n",
    "        'davies_bouldin': float(davies_bouldin_score(Xt, labels)),\n",
    "        'calinski_harabasz': float(calinski_harabasz_score(Xt, labels)),\n",
    "        'inertia': float(km.inertia_)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa260f1c",
   "metadata": {},
   "source": [
    "## 4. Run the comparison\n",
    "For each scaler â†’ scale the data â†’ run K-Means (k=20) â†’ compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98cce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "for name, scaler in SCALERS.items():\n",
    "    Xt = scaler.fit_transform(X)\n",
    "    met = kmeans_metrics(Xt, k=20, random_state=42)\n",
    "    rows.append({'scaler': name, **met})\n",
    "\n",
    "results = pd.DataFrame(rows).sort_values(['silhouette'], ascending=False)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed19ba1",
   "metadata": {},
   "source": [
    "**How to read this table**\n",
    "- **Higher Silhouette** â†’ tighter, more separated clusters\n",
    "- **Lower Daviesâ€“Bouldin** â†’ better separation\n",
    "- **Higher Calinskiâ€“Harabasz** â†’ denser, more compact clusters\n",
    "\n",
    "ðŸ‘‰ If your data is skewed, **QuantileTransformer** often performs well. **RobustScaler** can help when there are outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41517c9",
   "metadata": {},
   "source": [
    "## 5. Visual effect of scaling (side-by-side histograms)\n",
    "Weâ€™ll pick one or two skewed features and plot *before vs after* scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose a feature to visualize\n",
    "feat = 'acousticness' if 'acousticness' in X.columns else available[0]\n",
    "\n",
    "# Raw\n",
    "plt.figure()\n",
    "X[feat].hist(bins=40)\n",
    "plt.title(f'Histogram (raw): {feat}')\n",
    "plt.xlabel(feat); plt.ylabel('count')\n",
    "plt.show()\n",
    "\n",
    "# Quantile-transformed\n",
    "qt = SCALERS['QuantileTransformer']\n",
    "Xt_q = qt.fit_transform(X)\n",
    "# Work with the single column of interest\n",
    "import numpy as np\n",
    "feat_idx = list(X.columns).index(feat)\n",
    "plt.figure()\n",
    "pd.Series(Xt_q[:, feat_idx]).hist(bins=40)\n",
    "plt.title(f'Histogram (Quantile): {feat}')\n",
    "plt.xlabel(f'{feat} (quantile scaled)'); plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e6b71d",
   "metadata": {},
   "source": [
    "**What to look for**\n",
    "- If the raw histogram has a long tail or lots of values near 0/1, the quantile-scaled version should look more symmetric.\n",
    "This helps distance-based clustering treat features more fairly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69c9f9",
   "metadata": {},
   "source": [
    "## 6. Save this comparison for the report (optional)\n",
    "Stores the table so later notebooks (or README) can cite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c148f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUT = Path(\"../reports\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "out_path = OUT / \"scaler_comparison_kmeans20.csv\"\n",
    "results.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef7ad38",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Takeaways\n",
    "- Scaling is not optional for distance-based clustering.\n",
    "- **QuantileTransformer** often wins on skewed features; **RobustScaler** helps with outliers; **Standard/MinMax** are solid defaults.\n",
    "- Keep `k` fixed while comparing scalers. Then, in the **next notebook**, sweep `k` to choose the number of playlists.\n",
    "\n",
    "**Next:** Open `3_analysing_k_means__choosing_k_Spotify_5000.ipynb` (rewrite) to run **Elbow** and **Silhouette vs. k** and pick a good `k`."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
