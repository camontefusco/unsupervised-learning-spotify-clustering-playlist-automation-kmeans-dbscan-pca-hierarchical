{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2641608e",
   "metadata": {},
   "source": [
    "# 5 — DBSCAN (Density-Based Clustering) on Spotify (Beginner-Friendly)\n",
    "\n",
    "**Goal:** Use DBSCAN to discover clusters without pre-setting `k`, and identify **outliers** (noise) among songs.\n",
    "\n",
    "**You’ll learn:**\n",
    "- What DBSCAN’s `eps` and `min_samples` mean (intuitively)\n",
    "- How to grid-search a few settings and interpret **noise%** and **#clusters**\n",
    "- How to compute Silhouette / Davies–Bouldin / Calinski–Harabasz when valid\n",
    "- How to visualize a couple of best settings on a PCA 2D map\n",
    "\n",
    "> DBSCAN shines when clusters are **irregular shapes** and there are **true outliers** to be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9ce82e",
   "metadata": {},
   "source": [
    "## 0) Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7,5)\n",
    "RNG = np.random.RandomState(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917dca03",
   "metadata": {},
   "source": [
    "## 1) Load and prepare the data\n",
    "We’ll clean column names and focus on Spotify audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b99a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA = Path(\"../data/spotify_5000_songs.csv\")\n",
    "assert DATA.exists(), f\"Missing data at {DATA}. Place your CSV there.\"\n",
    "\n",
    "def clean_col(c):\n",
    "    s = re.sub(r\"\\s+\", \" \", str(c)).strip()\n",
    "    return s.split(\" \")[0]\n",
    "\n",
    "df_raw = pd.read_csv(DATA)\n",
    "df = df_raw.copy()\n",
    "df.columns = [clean_col(c) for c in df.columns]\n",
    "\n",
    "FEATURES = ['danceability','energy','acousticness','instrumentalness','liveness','valence',\n",
    "            'tempo','speechiness','loudness','duration_ms','key','mode','time_signature']\n",
    "available = [c for c in FEATURES if c in df.columns]\n",
    "X = df[available].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "print('Using features:', available, '| Shape:', X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67559c5d",
   "metadata": {},
   "source": [
    "## 2) Scale before DBSCAN\n",
    "DBSCAN uses distances under the hood. We’ll use **QuantileTransformer** (good for skew). You can toggle **StandardScaler** to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e2a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_quantile = True\n",
    "\n",
    "if use_quantile:\n",
    "    scaler = QuantileTransformer(output_distribution='normal', n_quantiles=min(1000, len(X)), random_state=42)\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "Xt = scaler.fit_transform(X)\n",
    "Xt[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16838b3",
   "metadata": {},
   "source": [
    "## 3) DBSCAN parameters (intuition)\n",
    "- **`eps`**: neighborhood radius — larger means more points considered ‘neighbors’ → fewer, bigger clusters\n",
    "- **`min_samples`**: how many points are needed to form a dense region — larger means more conservative clustering\n",
    "\n",
    "**Heuristics:**\n",
    "- Start with small `eps` and increase until structure appears\n",
    "- Try `min_samples` in `{5, 10, 20}` (density tolerance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b492de",
   "metadata": {},
   "source": [
    "## 4) Try a small grid of settings and score results\n",
    "We’ll compute **Silhouette**, **Davies–Bouldin**, **Calinski–Harabasz** when at least two clusters are found. We also track **noise%** and **#clusters** (excluding noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_metrics(Xt, labels):\n",
    "    uniq = set(labels)\n",
    "    valid = [l for l in uniq if l != -1]\n",
    "    if len(valid) < 2:\n",
    "        return {'silhouette': None, 'davies_bouldin': None, 'calinski_harabasz': None}\n",
    "    return {\n",
    "        'silhouette': float(silhouette_score(Xt, labels)),\n",
    "        'davies_bouldin': float(davies_bouldin_score(Xt, labels)),\n",
    "        'calinski_harabasz': float(calinski_harabasz_score(Xt, labels)),\n",
    "    }\n",
    "\n",
    "grid_eps = [0.5, 0.8, 1.0, 1.2]\n",
    "grid_min = [5, 10, 20]\n",
    "\n",
    "rows = []\n",
    "for eps in grid_eps:\n",
    "    for ms in grid_min:\n",
    "        model = DBSCAN(eps=eps, min_samples=ms).fit(Xt)\n",
    "        labels = model.labels_\n",
    "        noise_pct = float(np.mean(labels==-1)) * 100.0\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        met = safe_metrics(Xt, labels)\n",
    "        rows.append({\n",
    "            'eps': eps, 'min_samples': ms,\n",
    "            'clusters': n_clusters,\n",
    "            'noise_pct': noise_pct,\n",
    "            **met\n",
    "        })\n",
    "\n",
    "df_grid = pd.DataFrame(rows).sort_values(['silhouette'], ascending=False)\n",
    "df_grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1732e107",
   "metadata": {},
   "source": [
    "**How to read this table**\n",
    "- **silhouette**: higher is better (only when ≥2 clusters)\n",
    "- **noise_pct**: % of songs labeled as outliers (too high may be impractical)\n",
    "- **clusters**: if 0–1, DBSCAN didn’t find structure at those settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e27738",
   "metadata": {},
   "source": [
    "## 5) Visualize a couple of settings on a PCA 2D map\n",
    "We’ll pick:\n",
    "- the **best** setting by Silhouette (if available)\n",
    "- a **contrasting** setting (e.g., with much higher noise%)\n",
    "So you can *see* the effect of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ccba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare a 2D PCA projection for plotting only\n",
    "from sklearn.decomposition import PCA\n",
    "P = PCA(n_components=2, random_state=42).fit_transform(Xt)\n",
    "\n",
    "def plot_dbscan(eps, ms, title_suffix=''):\n",
    "    model = DBSCAN(eps=eps, min_samples=ms).fit(Xt)\n",
    "    labels = model.labels_\n",
    "    plt.figure()\n",
    "    plt.scatter(P[:,0], P[:,1], c=labels, s=4)\n",
    "    plt.title(f'DBSCAN eps={eps}, min_samples={ms} {title_suffix}')\n",
    "    plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "    plt.show()\n",
    "\n",
    "# Plot best silhouette combo (if exists)\n",
    "best = df_grid.dropna(subset=['silhouette']).sort_values('silhouette', ascending=False).head(1)\n",
    "if len(best):\n",
    "    b = best.iloc[0]\n",
    "    plot_dbscan(b['eps'], int(b['min_samples']), '(best silhouette)')\n",
    "\n",
    "# Plot a high-noise case (if available)\n",
    "hi_noise = df_grid.sort_values('noise_pct', ascending=False).head(1)\n",
    "if len(hi_noise):\n",
    "    h = hi_noise.iloc[0]\n",
    "    plot_dbscan(h['eps'], int(h['min_samples']), '(high noise)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf81c40",
   "metadata": {},
   "source": [
    "**Reading the plots**\n",
    "- Large grey-ish (–1) regions would indicate many outliers (if your viewer maps -1 to a single color)\n",
    "- A small number of big color islands → larger clusters\n",
    "- Many little fragments → overly sensitive parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e347a8",
   "metadata": {},
   "source": [
    "## 6) Save results for your report\n",
    "We’ll save the grid and a short ‘best rows’ table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUT = Path(\"../reports\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "grid_path = OUT / \"dbscan_grid_results.csv\"\n",
    "df_grid.to_csv(grid_path, index=False)\n",
    "\n",
    "best_path = OUT / \"dbscan_best_rows.csv\"\n",
    "df_grid.dropna(subset=['silhouette']).sort_values('silhouette', ascending=False).head(5).to_csv(best_path, index=False)\n",
    "\n",
    "print(\"Saved:\", grid_path)\n",
    "print(\"Saved:\", best_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd95f9",
   "metadata": {},
   "source": [
    "---\n",
    "## 7) When DBSCAN helps (and when it doesn’t)\n",
    "- ✅ **Finds outliers** naturally (songs that don’t fit any mood cluster)\n",
    "- ✅ Can find **arbitrary-shaped** clusters (not just spheres)\n",
    "- ⚠️ Sensitive to `eps`/`min_samples`; you’ll often get either **too much noise** or **one giant cluster**\n",
    "- ⚠️ On moderately uniform datasets, centroid/hierarchical methods can perform more consistently\n",
    "\n",
    "**Moosic takeaway:** Use DBSCAN **in addition** to K-Means/Agglomerative when you want to **flag oddball tracks** and prototype ‘edge-case’ playlists (e.g., experimental, live-only)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
