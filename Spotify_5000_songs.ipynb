{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a41bde",
   "metadata": {},
   "source": [
    "# Spotify 5000 Songs — Data Overview & Cleaning (Beginner-Friendly)\n",
    "\n",
    "**Goal:** Understand the dataset, tidy column names, check basic quality (missing values, duplicates), and save a **cleaned CSV** for the modeling notebooks.\n",
    "\n",
    "**You’ll learn:**\n",
    "- What each column looks like (types, ranges)\n",
    "- How to spot missing values and duplicates\n",
    "- How to quickly inspect feature distributions and correlations\n",
    "- How to save a clean, reproducible dataset for later steps\n",
    "\n",
    "> Tip: This notebook is safe to share on GitHub (no secrets), and it writes clean artifacts into `../data/` and `../reports/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ec00f",
   "metadata": {},
   "source": [
    "## 0) Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b772a57",
   "metadata": {},
   "source": [
    "## 1) Load CSV and clean column names\n",
    "Place your raw file at `../data/spotify_5000_songs.csv`. We’ll:\n",
    "- collapse repeated spaces\n",
    "- trim whitespace\n",
    "- keep the **first token** in any oddly-formatted header (works for exports where `name` has extra suffixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d67e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA = Path(\"../data/spotify_5000_songs.csv\")\n",
    "assert DATA.exists(), f\"Missing data at {DATA}. Place your CSV there.\"\n",
    "\n",
    "raw = pd.read_csv(DATA)\n",
    "\n",
    "def clean_col(c):\n",
    "    s = re.sub(r\"\\s+\", \" \", str(c)).strip()\n",
    "    return s.split(\" \")[0]\n",
    "\n",
    "df = raw.copy()\n",
    "df.columns = [clean_col(c) for c in df.columns]\n",
    "\n",
    "print(\"Rows, Cols:\", df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287dc86",
   "metadata": {},
   "source": [
    "## 2) Column types & quick dictionary\n",
    "This helps you see which fields are numeric vs. text and what we’ll likely use for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A quick 'data dictionary' starter based on column names\n",
    "possible_id_cols = [c for c in df.columns if c.lower() in {'id','track_id','uri'} or 'id' in c.lower()]\n",
    "possible_name_cols = [c for c in df.columns if c.lower() in {'name','song_name','track','title'}]\n",
    "possible_artist_cols = [c for c in df.columns if 'artist' in c.lower()]\n",
    "possible_numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "data_dict = pd.DataFrame({\n",
    "    'column': df.columns,\n",
    "    'dtype': [df[c].dtype for c in df.columns],\n",
    "    'example': [df[c].dropna().iloc[0] if df[c].notna().any() else None for c in df.columns],\n",
    "    'note': [\n",
    "        'identifier' if c in possible_id_cols else\n",
    "        'track name' if c in possible_name_cols else\n",
    "        'artist' if c in possible_artist_cols else\n",
    "        ('numeric feature' if c in possible_numeric_cols else 'other')\n",
    "        for c in df.columns\n",
    "    ]\n",
    "})\n",
    "data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66ba41",
   "metadata": {},
   "source": [
    "## 3) Basic quality checks: missing values & duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b1ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "missing[missing > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133110ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check duplicates by track 'id' if present, else by (name, artist) combo\n",
    "dup_mask = None\n",
    "if any(c in df.columns for c in ['id','track_id']):\n",
    "    key = 'id' if 'id' in df.columns else 'track_id'\n",
    "    dup_mask = df.duplicated(subset=[key], keep=False)\n",
    "else:\n",
    "    name_col = next((c for c in ['name','song_name','title','track'] if c in df.columns), None)\n",
    "    artist_col = next((c for c in df.columns if 'artist' in c.lower()), None)\n",
    "    if name_col and artist_col:\n",
    "        dup_mask = df.duplicated(subset=[name_col, artist_col], keep=False)\n",
    "\n",
    "if dup_mask is not None:\n",
    "    duplicates = df[dup_mask].sort_values(list(df.columns)[:3]).head(20)\n",
    "    print(\"Duplicate candidates (showing up to 20):\")\n",
    "    display(duplicates)\n",
    "else:\n",
    "    print(\"No suitable key for duplicate detection was found; skipping duplicate listing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1d45e",
   "metadata": {},
   "source": [
    "## 4) Feature distributions (numeric)\n",
    "We’ll look at summary stats and a few histograms to spot skew/outliers.\n",
    "Pick 4–6 representative features to keep the notebook fast and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c895f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "desc = df[num_cols].describe().T\n",
    "desc[['mean','std','min','25%','50%','75%','max']].round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e28695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "to_plot = [c for c in ['energy','valence','tempo','duration_ms','acousticness','instrumentalness','loudness'] if c in num_cols][:6]\n",
    "for c in to_plot:\n",
    "    plt.figure()\n",
    "    df[c].dropna().hist(bins=40)\n",
    "    plt.title(f'Histogram: {c}')\n",
    "    plt.xlabel(c); plt.ylabel('count')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a6f2d",
   "metadata": {},
   "source": [
    "## 5) Correlations (numeric features)\n",
    "This can hint which features move together (e.g., **energy** and **loudness**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20765cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr = df[num_cols].corr(numeric_only=True)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(corr, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='correlation')\n",
    "plt.xticks(range(len(num_cols)), num_cols, rotation=90)\n",
    "plt.yticks(range(len(num_cols)), num_cols)\n",
    "plt.title('Correlation heatmap (numeric features)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f131d0",
   "metadata": {},
   "source": [
    "**How to read the heatmap**\n",
    "- Bright (towards 1): features rise/fall together (e.g., energy & loudness)\n",
    "- Dark (towards -1): features move in opposite directions\n",
    "- Near 0: little linear relationship\n",
    "\n",
    "This helps interpret PCA axes and cluster dimensions later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0cbdc1",
   "metadata": {},
   "source": [
    "## 6) Optional quick outlier look (z-score)\n",
    "We’ll compute simple z-scores and list rows with extreme values for a couple of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ea648",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "z_df = df[num_cols].apply(lambda s: zscore(s.dropna()), raw=False)\n",
    "# Align back (zscore drops NaNs, so reindex and keep only finite)\n",
    "z_df = z_df.reindex(df.index)\n",
    "extreme_cols = [c for c in ['tempo','duration_ms','loudness'] if c in num_cols]\n",
    "outlier_rows = {}\n",
    "for c in extreme_cols:\n",
    "    if c in z_df.columns:\n",
    "        z = z_df[c]\n",
    "        outlier_rows[c] = df[(z.abs() >= 4)].head(10)[[c] + [col for col in ['name','song_name','artist'] if col in df.columns]]\n",
    "outlier_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e477af4",
   "metadata": {},
   "source": [
    "## 7) Save cleaned artifacts\n",
    "We’ll write a cleaned CSV and summary tables to reuse across notebooks and for your GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_OUT = Path(\"../data/spotify_5000_songs.cleaned.csv\")\n",
    "REPORTS = Path(\"../reports\"); REPORTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_csv(DATA_OUT, index=False)\n",
    "desc_out = REPORTS / \"data_summary_numeric.csv\"\n",
    "desc.to_csv(desc_out)\n",
    "\n",
    "dict_out = REPORTS / \"data_dictionary_autogen.csv\"\n",
    "data_dict.to_csv(dict_out, index=False)\n",
    "\n",
    "print(\"Saved cleaned CSV:\", DATA_OUT)\n",
    "print(\"Saved numeric summary:\", desc_out)\n",
    "print(\"Saved dictionary (auto):\", dict_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d2ccf",
   "metadata": {},
   "source": [
    "---\n",
    "## 8) Takeaways & next steps\n",
    "- Now you have a **cleaned CSV** for consistent runs\n",
    "- You saw overall distributions and correlations → intuition for PCA and clustering\n",
    "- If particular features are very skewed, **QuantileTransformer** will likely help\n",
    "\n",
    "**Continue with:**\n",
    "- `1_introduction_to_kmeans_Spotify_5000_REWRITE.ipynb` (first K-Means draft)\n",
    "- `2_scaling_data_Spotify_5000_REWRITE.ipynb` (scaler comparison)\n",
    "- `3_analysing_k_means__choosing_k_Spotify_5000_REWRITE.ipynb` (choose k)\n",
    "- `4_PCA_Spotify_5000_REWRITE.ipynb`, `5_DBSCAN_Spotify_5000_REWRITE.ipynb`, `6_AgglomerativeClustering_Spotify_5000_REWRITE.ipynb`"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
